{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d213c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from collections import Counter\n",
    "\n",
    "# ML & utilities \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()  \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8013a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 736 total image files\n"
     ]
    }
   ],
   "source": [
    "def load_pickle_image(file_path):\n",
    "    with open(file_path, \"rb\") as f:         \n",
    "        return pickle.load(f)                 \n",
    "\n",
    "file_paths = []                              \n",
    "for vol in range(1, 9):                      \n",
    "    folder = f\"archive/vol{vol:02d}\"         \n",
    "    file_paths += sorted(glob.glob(f\"{folder}/*.pck\"))  \n",
    "\n",
    "print(f\"Found {len(file_paths)} total image files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ba8897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution (numeric): Counter({0: 547, 1: 144, 2: 45})\n",
      "Label distribution (names): Counter({'Normal': 547, 'Torn': 144, 'Partially torn': 45})\n"
     ]
    }
   ],
   "source": [
    "# Associate labels from metadata.csv to image file paths\n",
    "\n",
    "meta = pd.read_csv(\"metadata.csv\")\n",
    "# Map filename -> numeric label\n",
    "fn_to_label = dict(zip(meta['volumeFilename'], meta['aclDiagnosis']))\n",
    "label_names = {0: 'Normal', 1: 'Torn', 2: 'Partially torn'}\n",
    "\n",
    "# Build mapping list (file_path, numeric_label, label_name_or_None)\n",
    "mapped = []\n",
    "for p in file_paths:\n",
    "    fname = os.path.basename(p)\n",
    "    label = fn_to_label.get(fname, None)\n",
    "    mapped.append((p, label, label_names.get(label) if label is not None else None))\n",
    "\n",
    "\n",
    "# Summary\n",
    "print(\"Label distribution (numeric):\", Counter(l for _p, l, _n in mapped if l is not None))\n",
    "print(\"Label distribution (names):\", Counter(n for _p, l, n in mapped if n is not None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa2981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done — 300 items (failed 0) in 133.3s\n",
      "Class weights: {0: 1.0, 1: 1.0, 2: 1.0}\n",
      "\n",
      "SVM acc: 0.5833333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.70      0.60        20\n",
      "           1       0.58      0.35      0.44        20\n",
      "           2       0.67      0.70      0.68        20\n",
      "\n",
      "    accuracy                           0.58        60\n",
      "   macro avg       0.59      0.58      0.57        60\n",
      "weighted avg       0.59      0.58      0.57        60\n",
      "\n",
      "\n",
      "LogReg acc: 0.6333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.60      0.60        20\n",
      "           1       0.69      0.45      0.55        20\n",
      "           2       0.63      0.85      0.72        20\n",
      "\n",
      "    accuracy                           0.63        60\n",
      "   macro avg       0.64      0.63      0.62        60\n",
      "weighted avg       0.64      0.63      0.62        60\n",
      "\n",
      "\n",
      "Training small CNN (5 epochs) with class weights...\n",
      "Epoch 1/5\n",
      "15/15 - 1s - 48ms/step - accuracy: 0.3292 - loss: 1.0997 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 2/5\n",
      "15/15 - 1s - 48ms/step - accuracy: 0.3292 - loss: 1.0997 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 2/5\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.2958 - loss: 1.0989 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 3/5\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.2958 - loss: 1.0989 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 3/5\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3083 - loss: 1.0987 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 4/5\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3083 - loss: 1.0987 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 4/5\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3250 - loss: 1.0987 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 5/5\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3250 - loss: 1.0987 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 5/5\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3167 - loss: 1.0987 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3167 - loss: 1.0987 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "\n",
      "CNN test acc: 0.3333333432674408\n",
      "\n",
      "CNN test acc: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "# Improved balanced training with simple augmentations to address class imbalance\n",
    "MAX_SAMPLES = 300\n",
    "IMG_SIZE = (64, 64)\n",
    "hog_params = dict(orientations=9, pixels_per_cell=(8,8), cells_per_block=(2,2))\n",
    "AUG_PER_SAMPLE = 1  # augment each drawn sample once when needed\n",
    "\n",
    "# Build per-label lists\n",
    "by_label = {}\n",
    "for p, label, _n in mapped:\n",
    "    if label is None:\n",
    "        continue\n",
    "    by_label.setdefault(label, []).append(p)\n",
    "\n",
    "classes = sorted(by_label.keys())\n",
    "n_classes = len(classes)\n",
    "if n_classes == 0:\n",
    "    raise RuntimeError('No labeled samples found')\n",
    "target_per_class = max(1, MAX_SAMPLES // n_classes)\n",
    "\n",
    "# Helper augmentation (works on resized 2D slices)\n",
    "from skimage.transform import rotate as _rotate\n",
    "def augment_slice(img):\n",
    "    a = img.copy()\n",
    "    if random.random() < 0.5:\n",
    "        a = np.fliplr(a)\n",
    "    ang = random.uniform(-10, 10)\n",
    "    a = _rotate(a, ang, mode='edge', preserve_range=True)\n",
    "    return a\n",
    "\n",
    "fds = []\n",
    "imgs = []\n",
    "ys = []\n",
    "failed = []\n",
    "t0 = time.time()\n",
    "for label in classes:\n",
    "    paths = by_label[label]\n",
    "    if len(paths) == 0:\n",
    "        continue\n",
    "    # Sample with replacement until we reach target_per_class\n",
    "    while sum(1 for y in ys if y == label) < target_per_class:\n",
    "        p = random.choice(paths)\n",
    "        try:\n",
    "            img = load_pickle_image(p)\n",
    "        except Exception as e:\n",
    "            failed.append((p, str(e)))\n",
    "            continue\n",
    "        arr = np.asarray(img)\n",
    "        sl = arr[arr.shape[0] // 2] if arr.ndim == 3 else arr\n",
    "        if sl.ndim == 3:\n",
    "            sl = rgb2gray(sl)\n",
    "        sl_resized = resize(sl, IMG_SIZE, anti_aliasing=True)\n",
    "        # original\n",
    "        fd = hog((sl_resized * 255).astype(np.uint8), orientations=hog_params['orientations'], pixels_per_cell=hog_params['pixels_per_cell'], cells_per_block=hog_params['cells_per_block'], visualize=False, feature_vector=True)\n",
    "        fds.append(fd)\n",
    "        imgs.append(np.expand_dims(sl_resized.astype(np.float32), -1))\n",
    "        ys.append(label)\n",
    "        # augmentation if still need more for this class\n",
    "        if sum(1 for y in ys if y == label) < target_per_class:\n",
    "            for _ in range(AUG_PER_SAMPLE):\n",
    "                aug = augment_slice(sl_resized)\n",
    "                fd2 = hog((aug * 255).astype(np.uint8), orientations=hog_params['orientations'], pixels_per_cell=hog_params['pixels_per_cell'], cells_per_block=hog_params['cells_per_block'], visualize=False, feature_vector=True)\n",
    "                fds.append(fd2)\n",
    "                imgs.append(np.expand_dims(aug.astype(np.float32), -1))\n",
    "                ys.append(label)\n",
    "t1 = time.time()\n",
    "print(f'Preprocessing done — {len(fds)} items (failed {len(failed)}) in {t1-t0:.1f}s')\n",
    "\n",
    "# Truncate or pad to MAX_SAMPLES\n",
    "if len(fds) > MAX_SAMPLES:\n",
    "    idx = random.sample(range(len(fds)), MAX_SAMPLES)\n",
    "    fds = [fds[i] for i in idx]\n",
    "    imgs = [imgs[i] for i in idx]\n",
    "    ys = [ys[i] for i in idx]\n",
    "\n",
    "X = np.vstack(fds)\n",
    "y = np.array(ys)\n",
    "X_img = np.stack(imgs)\n",
    "y_img = np.array(ys)\n",
    "\n",
    "# Compute class weights for CNN\n",
    "from sklearn.utils import class_weight\n",
    "unique = np.unique(y_img)\n",
    "cw = class_weight.compute_class_weight('balanced', classes=unique, y=y_img)\n",
    "class_weight_dict = {int(c): float(w) for c, w in zip(unique, cw)}\n",
    "print('Class weights:', class_weight_dict)\n",
    "\n",
    "# Train/test split for HOG features\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# SVM (balanced)\n",
    "svm = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
    "svm.fit(X_tr, y_tr)\n",
    "y_svm = svm.predict(X_te)\n",
    "print('\\nSVM acc:', accuracy_score(y_te, y_svm))\n",
    "print(classification_report(y_te, y_svm))\n",
    "\n",
    "# Logistic Regression\n",
    "logr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "logr.fit(X_tr, y_tr)\n",
    "y_log = logr.predict(X_te)\n",
    "print('\\nLogReg acc:', accuracy_score(y_te, y_log))\n",
    "print(classification_report(y_te, y_log))\n",
    "\n",
    "# CNN training with class weights and small augmentations baked in\n",
    "Xtr_img, Xte_img, ytr_img, yte_img = train_test_split(X_img, y_img, test_size=0.2, stratify=y_img, random_state=42)\n",
    "n_classes = len(np.unique(y_img))\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=Xtr_img.shape[1:]),\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.Conv2D(16, 3, activation='relu'),\n",
    "    layers.MaxPool2D(),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPool2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print('\\nTraining small CNN (5 epochs) with class weights...')\n",
    "model.fit(Xtr_img, ytr_img, epochs=5, batch_size=16, validation_data=(Xte_img, yte_img), class_weight=class_weight_dict, verbose=2)\n",
    "loss, acc = model.evaluate(Xte_img, yte_img, verbose=0)\n",
    "print('\\nCNN test acc:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
