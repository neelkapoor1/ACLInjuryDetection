{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d213c204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x12aa04ce0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from collections import Counter\n",
    "\n",
    "# ML & utilities \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc77f85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 736 total image files\n"
     ]
    }
   ],
   "source": [
    "def load_pickle_image(file_path):\n",
    "    with open(file_path, \"rb\") as f:         \n",
    "        return pickle.load(f)                 \n",
    "\n",
    "file_paths = []                              \n",
    "for vol in range(1, 9):                      \n",
    "    folder = f\"archive/vol{vol:02d}\"         \n",
    "    file_paths += sorted(glob.glob(f\"{folder}/*.pck\"))  \n",
    "\n",
    "print(f\"Found {len(file_paths)} total image files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df32027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution (numeric): Counter({0: 547, 1: 144, 2: 45})\n",
      "Label distribution (names): Counter({'Normal': 547, 'Torn': 144, 'Partially torn': 45})\n"
     ]
    }
   ],
   "source": [
    "# Associate labels from metadata.csv to image file paths\n",
    "\n",
    "meta = pd.read_csv(\"metadata.csv\")\n",
    "# Map filename -> numeric label\n",
    "fn_to_label = dict(zip(meta['volumeFilename'], meta['aclDiagnosis']))\n",
    "label_names = {0: 'Normal', 1: 'Torn', 2: 'Partially torn'}\n",
    "\n",
    "# Build mapping list (file_path, numeric_label, label_name_or_None)\n",
    "mapped = []\n",
    "for p in file_paths:\n",
    "    fname = os.path.basename(p)\n",
    "    label = fn_to_label.get(fname, None)\n",
    "    mapped.append((p, label, label_names.get(label) if label is not None else None))\n",
    "\n",
    "\n",
    "# Summary\n",
    "print(\"Label distribution (numeric):\", Counter(l for _p, l, _n in mapped if l is not None))\n",
    "print(\"Label distribution (names):\", Counter(n for _p, l, n in mapped if n is not None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 1.0, 1: 1.0, 2: 1.0}\n",
      "SGD-SVM acc: 0.5333333333333333\n",
      "SGD-LogReg acc: 0.5\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neel/Documents/GitHub/ACLInjuryDetection/venv/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/neel/Documents/GitHub/ACLInjuryDetection/venv/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/neel/Documents/GitHub/ACLInjuryDetection/venv/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/neel/Documents/GitHub/ACLInjuryDetection/venv/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/neel/Documents/GitHub/ACLInjuryDetection/venv/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/neel/Documents/GitHub/ACLInjuryDetection/venv/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 - 1s - 45ms/step - accuracy: 0.2875 - loss: 1.0994 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 2/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3333 - loss: 1.0987 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 3/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.2875 - loss: 1.0988 - val_accuracy: 0.3500 - val_loss: 1.0986\n",
      "Epoch 4/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3333 - loss: 1.0986 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 5/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3333 - loss: 1.0987 - val_accuracy: 0.3333 - val_loss: 1.0986\n",
      "Epoch 6/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.2708 - loss: 1.0988 - val_accuracy: 0.3167 - val_loss: 1.0986\n",
      "Epoch 7/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3167 - loss: 1.0988 - val_accuracy: 0.4333 - val_loss: 1.0984\n",
      "Epoch 8/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3750 - loss: 1.0984 - val_accuracy: 0.3000 - val_loss: 1.0984\n",
      "Epoch 9/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3875 - loss: 1.0974 - val_accuracy: 0.3667 - val_loss: 1.0987\n",
      "Epoch 10/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.3917 - loss: 1.0956 - val_accuracy: 0.3333 - val_loss: 1.0996\n",
      "Epoch 11/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.4167 - loss: 1.0903 - val_accuracy: 0.3500 - val_loss: 1.0968\n",
      "Epoch 12/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.4083 - loss: 1.0849 - val_accuracy: 0.3833 - val_loss: 1.0985\n",
      "Epoch 13/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.4458 - loss: 1.0730 - val_accuracy: 0.3667 - val_loss: 1.0962\n",
      "Epoch 14/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.4625 - loss: 1.0485 - val_accuracy: 0.3833 - val_loss: 1.0882\n",
      "Epoch 15/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.5875 - loss: 1.0056 - val_accuracy: 0.4167 - val_loss: 1.1136\n",
      "Epoch 16/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.5583 - loss: 0.9863 - val_accuracy: 0.4667 - val_loss: 1.1198\n",
      "Epoch 17/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.5417 - loss: 0.9306 - val_accuracy: 0.4667 - val_loss: 1.1858\n",
      "Epoch 18/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.5625 - loss: 0.8922 - val_accuracy: 0.4667 - val_loss: 1.1822\n",
      "Epoch 19/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.5917 - loss: 0.8530 - val_accuracy: 0.5000 - val_loss: 1.2260\n",
      "Epoch 20/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.6042 - loss: 0.8196 - val_accuracy: 0.5000 - val_loss: 1.2275\n",
      "Epoch 21/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.6292 - loss: 0.8018 - val_accuracy: 0.5000 - val_loss: 1.2143\n",
      "Epoch 22/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.6792 - loss: 0.7659 - val_accuracy: 0.5500 - val_loss: 1.2878\n",
      "Epoch 23/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.7000 - loss: 0.7160 - val_accuracy: 0.5833 - val_loss: 1.2434\n",
      "Epoch 24/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.6792 - loss: 0.7467 - val_accuracy: 0.5167 - val_loss: 1.1841\n",
      "Epoch 25/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.6917 - loss: 0.7314 - val_accuracy: 0.5833 - val_loss: 1.3214\n",
      "Epoch 26/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.7375 - loss: 0.6435 - val_accuracy: 0.5500 - val_loss: 1.3495\n",
      "Epoch 27/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.7625 - loss: 0.6396 - val_accuracy: 0.5333 - val_loss: 1.1818\n",
      "Epoch 28/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.7125 - loss: 0.6675 - val_accuracy: 0.6000 - val_loss: 1.3556\n",
      "Epoch 29/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.7458 - loss: 0.5950 - val_accuracy: 0.6333 - val_loss: 1.2775\n",
      "Epoch 30/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.7542 - loss: 0.5977 - val_accuracy: 0.6000 - val_loss: 1.3773\n",
      "Epoch 31/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.7833 - loss: 0.5738 - val_accuracy: 0.6000 - val_loss: 1.3277\n",
      "Epoch 32/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.8125 - loss: 0.5180 - val_accuracy: 0.6000 - val_loss: 1.3592\n",
      "Epoch 33/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.8250 - loss: 0.4928 - val_accuracy: 0.6333 - val_loss: 1.4429\n",
      "Epoch 34/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.8125 - loss: 0.4720 - val_accuracy: 0.6333 - val_loss: 1.3421\n",
      "Epoch 35/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.8167 - loss: 0.4657 - val_accuracy: 0.6000 - val_loss: 1.4081\n",
      "Epoch 36/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.8375 - loss: 0.4251 - val_accuracy: 0.6000 - val_loss: 1.4140\n",
      "Epoch 37/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.8250 - loss: 0.4426 - val_accuracy: 0.6167 - val_loss: 1.3383\n",
      "Epoch 38/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.8458 - loss: 0.3947 - val_accuracy: 0.6500 - val_loss: 1.4798\n",
      "Epoch 39/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.8667 - loss: 0.4107 - val_accuracy: 0.6000 - val_loss: 1.4965\n",
      "Epoch 40/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.8125 - loss: 0.4302 - val_accuracy: 0.5833 - val_loss: 1.5573\n",
      "Epoch 41/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.8875 - loss: 0.3580 - val_accuracy: 0.6500 - val_loss: 1.4952\n",
      "Epoch 42/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.9042 - loss: 0.3133 - val_accuracy: 0.6167 - val_loss: 1.5658\n",
      "Epoch 43/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.8875 - loss: 0.2907 - val_accuracy: 0.6500 - val_loss: 1.5269\n",
      "Epoch 44/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.9042 - loss: 0.2889 - val_accuracy: 0.6167 - val_loss: 1.5870\n",
      "Epoch 45/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.9167 - loss: 0.2579 - val_accuracy: 0.5667 - val_loss: 1.6590\n",
      "Epoch 46/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.9083 - loss: 0.2718 - val_accuracy: 0.6167 - val_loss: 1.7366\n",
      "Epoch 47/50\n",
      "15/15 - 0s - 5ms/step - accuracy: 0.9250 - loss: 0.2198 - val_accuracy: 0.6000 - val_loss: 1.6236\n",
      "Epoch 48/50\n",
      "15/15 - 0s - 7ms/step - accuracy: 0.9250 - loss: 0.2157 - val_accuracy: 0.5833 - val_loss: 1.8066\n",
      "Epoch 49/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.9542 - loss: 0.1927 - val_accuracy: 0.6000 - val_loss: 1.7314\n",
      "Epoch 50/50\n",
      "15/15 - 0s - 6ms/step - accuracy: 0.9542 - loss: 0.1945 - val_accuracy: 0.5833 - val_loss: 1.8282\n",
      "\n",
      "CNN test acc: 0.5833333134651184\n"
     ]
    }
   ],
   "source": [
    "MAX_SAMPLES = 300\n",
    "IMG_SIZE = (64, 64)\n",
    "hog_params = dict(orientations=9, pixels_per_cell=(8,8), cells_per_block=(2,2))\n",
    "AUG_PER_SAMPLE = 1\n",
    "\n",
    "# Build per-label lists\n",
    "paths_by_label = {}\n",
    "for file_path, label, _name in mapped:\n",
    "    if label is None:\n",
    "        continue\n",
    "    paths_by_label.setdefault(label, []).append(file_path)\n",
    "\n",
    "class_labels = sorted(paths_by_label.keys())\n",
    "num_classes = len(class_labels)\n",
    "samples_per_class_target = max(1, MAX_SAMPLES // num_classes)\n",
    "\n",
    "# Helper augmentation (works on resized 2D slices)\n",
    "from skimage.transform import rotate as _rotate\n",
    "def augment_slice(img):\n",
    "    a = img.copy()\n",
    "    if random.random() < 0.5:\n",
    "        a = np.fliplr(a)\n",
    "    ang = random.uniform(-10, 10)\n",
    "    a = _rotate(a, ang, mode='edge', preserve_range=True)\n",
    "    return a\n",
    "\n",
    "hog_features_list = []\n",
    "image_tensors_list = []\n",
    "labels_list = []\n",
    "failed_files = []\n",
    "t_start = time.time()\n",
    "for label in class_labels:\n",
    "    file_paths_for_label = paths_by_label[label]\n",
    "    if len(file_paths_for_label) == 0:\n",
    "        continue\n",
    "    # Sample with replacement until we reach the per-class target\n",
    "    while sum(1 for lab in labels_list if lab == label) < samples_per_class_target:\n",
    "        chosen_path = random.choice(file_paths_for_label)\n",
    "        volume = load_pickle_image(chosen_path)\n",
    "        volume_arr = np.asarray(volume)\n",
    "        mid_slice = volume_arr[volume_arr.shape[0] // 2] if volume_arr.ndim == 3 else volume_arr\n",
    "        if mid_slice.ndim == 3:\n",
    "            mid_slice = rgb2gray(mid_slice)\n",
    "        slice_resized = resize(mid_slice, IMG_SIZE, anti_aliasing=True)\n",
    "        # compute HOG descriptor for this slice\n",
    "        hog_descriptor = hog((slice_resized * 255).astype(np.uint8), orientations=hog_params['orientations'], pixels_per_cell=hog_params['pixels_per_cell'], cells_per_block=hog_params['cells_per_block'], visualize=False, feature_vector=True)\n",
    "        hog_features_list.append(hog_descriptor)\n",
    "        image_tensors_list.append(np.expand_dims(slice_resized.astype(np.float32), -1))\n",
    "        labels_list.append(label)\n",
    "        # perform light augmentations if we still need more examples for this class\n",
    "        if sum(1 for lab in labels_list if lab == label) < samples_per_class_target:\n",
    "            for _ in range(AUG_PER_SAMPLE):\n",
    "                aug_slice = augment_slice(slice_resized)\n",
    "                hog_descriptor_aug = hog((aug_slice * 255).astype(np.uint8), orientations=hog_params['orientations'], pixels_per_cell=hog_params['pixels_per_cell'], cells_per_block=hog_params['cells_per_block'], visualize=False, feature_vector=True)\n",
    "                hog_features_list.append(hog_descriptor_aug)\n",
    "                image_tensors_list.append(np.expand_dims(aug_slice.astype(np.float32), -1))\n",
    "                labels_list.append(label)\n",
    "t_end = time.time()\n",
    "\n",
    "# Truncate or pad to MAX_SAMPLES\n",
    "if len(hog_features_list) > MAX_SAMPLES:\n",
    "    keep_idx = random.sample(range(len(hog_features_list)), MAX_SAMPLES)\n",
    "    hog_features_list = [hog_features_list[i] for i in keep_idx]\n",
    "    image_tensors_list = [image_tensors_list[i] for i in keep_idx]\n",
    "    labels_list = [labels_list[i] for i in keep_idx]\n",
    "\n",
    "X_hog = np.vstack(hog_features_list)\n",
    "y_labels = np.array(labels_list)\n",
    "X_images = np.stack(image_tensors_list)\n",
    "y_image_labels = np.array(labels_list)\n",
    "\n",
    "# Compute class weights for CNN\n",
    "from sklearn.utils import class_weight\n",
    "unique_classes = np.unique(y_image_labels)\n",
    "cw = class_weight.compute_class_weight('balanced', classes=unique_classes, y=y_image_labels)\n",
    "class_weight_map = {int(c): float(w) for c, w in zip(unique_classes, cw)}\n",
    "print('Class weights:', class_weight_map)\n",
    "\n",
    "# Train/test split for HOG features\n",
    "X_hog_train, X_hog_test, y_hog_train, y_hog_test = train_test_split(X_hog, y_labels, test_size=0.2, stratify=y_labels, random_state=42)\n",
    "\n",
    "epochs = 10\n",
    "sgd_svm = SGDClassifier(loss=\"hinge\", class_weight=class_weight_map, random_state=42)\n",
    "for epoch in range(epochs):\n",
    "    sgd_svm.partial_fit(X_hog_train, y_hog_train, classes=np.unique(y_hog_train))\n",
    "\n",
    "sgd_logr = SGDClassifier(loss=\"log_loss\", class_weight=class_weight_map, random_state=42)\n",
    "for epoch in range(epochs):\n",
    "    sgd_logr.partial_fit(X_hog_train, y_hog_train, classes=np.unique(y_hog_train))\n",
    "\n",
    "print(\"SGD-SVM acc:\", accuracy_score(y_hog_test, sgd_svm.predict(X_hog_test)))\n",
    "print(\"SGD-LogReg acc:\", accuracy_score(y_hog_test, sgd_logr.predict(X_hog_test)))\n",
    "\n",
    "# CNN training with class weights and small augmentations baked in\n",
    "X_images_train, X_images_test, y_images_train, y_images_test = train_test_split(X_images, y_image_labels, test_size=0.2, stratify=y_image_labels, random_state=42)\n",
    "n_classes = len(np.unique(y_image_labels))\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=X_images_train.shape[1:]),\n",
    "    layers.Conv2D(16, 3, activation='relu'),\n",
    "    layers.MaxPool2D(),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPool2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_images_train, y_images_train, epochs=50, batch_size=16, validation_data=(X_images_test, y_images_test), class_weight=class_weight_map, verbose=2)\n",
    "loss, acc = model.evaluate(X_images_test, y_images_test, verbose=0)\n",
    "print('\\nCNN test acc:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
